2024-04-20 19:30:38,872 - root - INFO -I am the driver applications.........
2024-04-20 19:30:38,872 - root - INFO -Calling the spark object.........  
2024-04-20 19:30:40,344 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x101b3ca90>
2024-04-20 19:30:40,344 - root - INFO -validating the spark object
2024-04-20 19:30:42,147 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 20))
2024-04-20 19:30:42,147 - root - WARNING -validation done ! go ahead......
2024-04-20 19:30:42,148 - root - INFO -reading the file format is parquet 
2024-04-20 19:30:42,148 - Ingest - WARNING -load_file method started!...
2024-04-20 19:30:42,437 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-20 19:30:42,439 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-20 19:30:42,822 - root - INFO -validating the dataframe......
2024-04-20 19:30:42,823 - Ingest - WARNING -count the records df_city 
2024-04-20 19:30:43,019 - root - INFO -total number of records in the dataframe 28338
2024-04-20 19:30:43,020 - Ingest - WARNING -load_file method started!...
2024-04-20 19:30:45,485 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-20 19:30:45,487 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-20 19:30:45,603 - root - INFO -validating the dataframe......
2024-04-20 19:30:45,603 - Ingest - WARNING -count the records df_fact 
2024-04-20 19:30:45,992 - root - INFO -total number of records in the dataframe 1329329
2024-04-20 19:30:45,992 - root - INFO -data processing..........
2024-04-20 19:30:45,992 - root - WARNING -data_clean method() started ......
2024-04-20 19:30:45,992 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-20 19:30:46,004 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-20 19:30:46,014 - root - WARNING -Adding a new column to df_presc_sel
2024-04-20 19:30:46,019 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-20 19:30:46,151 - root - INFO -validating schema for dataframes.....
2024-04-20 19:30:46,152 - root - INFO -Applications name .......   
2024-04-20 19:31:40,319 - root - INFO -I am the driver applications.........
2024-04-20 19:31:40,319 - root - INFO -Calling the spark object.........  
2024-04-20 19:31:41,821 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x103470a90>
2024-04-20 19:31:41,821 - root - INFO -validating the spark object
2024-04-20 19:31:43,638 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 20))
2024-04-20 19:31:43,638 - root - WARNING -validation done ! go ahead......
2024-04-20 19:31:43,638 - root - INFO -reading the file format is parquet 
2024-04-20 19:31:43,638 - Ingest - WARNING -load_file method started!...
2024-04-20 19:31:44,031 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-20 19:31:44,033 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-20 19:31:44,410 - root - INFO -validating the dataframe......
2024-04-20 19:31:44,410 - Ingest - WARNING -count the records df_city 
2024-04-20 19:31:44,598 - root - INFO -total number of records in the dataframe 28338
2024-04-20 19:31:44,598 - Ingest - WARNING -load_file method started!...
2024-04-20 19:31:47,174 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-20 19:31:47,176 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-20 19:31:47,306 - root - INFO -validating the dataframe......
2024-04-20 19:31:47,306 - Ingest - WARNING -count the records df_fact 
2024-04-20 19:31:47,707 - root - INFO -total number of records in the dataframe 1329329
2024-04-20 19:31:47,708 - root - INFO -data processing..........
2024-04-20 19:31:47,708 - root - WARNING -data_clean method() started ......
2024-04-20 19:31:47,708 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-20 19:31:47,721 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-20 19:31:47,732 - root - WARNING -Adding a new column to df_presc_sel
2024-04-20 19:31:47,737 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-20 19:31:47,870 - root - INFO -validating schema for dataframes.....
2024-04-20 19:31:47,871 - root - INFO -Applications name .......   
2024-04-20 19:46:51,626 - root - INFO -I am the driver applications.........
2024-04-20 19:46:51,626 - root - INFO -Calling the spark object.........  
2024-04-20 19:46:53,130 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x10198ab00>
2024-04-20 19:46:53,130 - root - INFO -validating the spark object
2024-04-20 19:46:54,908 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 20))
2024-04-20 19:46:54,908 - root - WARNING -validation done ! go ahead......
2024-04-20 19:46:54,908 - root - INFO -reading the file format is parquet 
2024-04-20 19:46:54,908 - Ingest - WARNING -load_file method started!...
2024-04-20 19:46:55,222 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-20 19:46:55,224 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-20 19:46:55,969 - root - INFO -validating the dataframe......
2024-04-20 19:46:55,969 - Ingest - WARNING -count the records df_city 
2024-04-20 19:46:56,170 - root - INFO -total number of records in the dataframe 28338
2024-04-20 19:46:56,170 - Ingest - WARNING -load_file method started!...
2024-04-20 19:46:58,658 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-20 19:46:58,660 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-20 19:46:58,765 - root - INFO -validating the dataframe......
2024-04-20 19:46:58,766 - Ingest - WARNING -count the records df_fact 
2024-04-20 19:46:59,228 - root - INFO -total number of records in the dataframe 1329329
2024-04-20 19:46:59,228 - root - INFO -data processing..........
2024-04-20 19:46:59,228 - root - WARNING -data_clean method() started ......
2024-04-20 19:46:59,228 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-20 19:46:59,247 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-20 19:46:59,261 - root - WARNING -Adding a new column to df_presc_sel
2024-04-20 19:46:59,268 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-20 19:46:59,288 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-20 19:46:59,300 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-20 19:46:59,306 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-20 19:46:59,477 - root - INFO -validating schema for dataframes.....
2024-04-20 19:46:59,601 - root - INFO -Applications name .......   
2024-04-20 19:51:25,993 - root - INFO -I am the driver applications.........
2024-04-20 19:51:25,994 - root - INFO -Calling the spark object.........  
2024-04-20 19:51:28,163 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x101d129e0>
2024-04-20 19:51:28,163 - root - INFO -validating the spark object
2024-04-20 19:51:30,244 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 20))
2024-04-20 19:51:30,244 - root - WARNING -validation done ! go ahead......
2024-04-20 19:51:30,245 - root - INFO -reading the file format is parquet 
2024-04-20 19:51:30,245 - Ingest - WARNING -load_file method started!...
2024-04-20 19:51:30,587 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-20 19:51:30,589 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-20 19:51:30,985 - root - INFO -validating the dataframe......
2024-04-20 19:51:30,985 - Ingest - WARNING -count the records df_city 
2024-04-20 19:51:31,204 - root - INFO -total number of records in the dataframe 28338
2024-04-20 19:51:31,204 - Ingest - WARNING -load_file method started!...
2024-04-20 19:51:33,742 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-20 19:51:33,743 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-20 19:51:33,863 - root - INFO -validating the dataframe......
2024-04-20 19:51:33,863 - Ingest - WARNING -count the records df_fact 
2024-04-20 19:51:34,236 - root - INFO -total number of records in the dataframe 1329329
2024-04-20 19:51:34,237 - root - INFO -data processing..........
2024-04-20 19:51:34,237 - root - WARNING -data_clean method() started ......
2024-04-20 19:51:34,237 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-20 19:51:34,250 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-20 19:51:34,259 - root - WARNING -Adding a new column to df_presc_sel
2024-04-20 19:51:34,264 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-20 19:51:34,278 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-20 19:51:34,291 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-20 19:51:34,297 - root - WARNING -Checking the null values in all columns
2024-04-20 19:51:34,364 - root - WARNING -drop the null values in the respective columns.......
2024-04-20 19:51:34,364 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-20 19:51:44,493 - root - INFO -validating schema for dataframes.....
2024-04-20 19:51:44,494 - root - INFO -Applications name .......   
2024-04-20 19:53:58,994 - root - INFO -I am the driver applications.........
2024-04-20 19:53:58,995 - root - INFO -Calling the spark object.........  
2024-04-20 19:54:00,590 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x1015daaa0>
2024-04-20 19:54:00,590 - root - INFO -validating the spark object
2024-04-20 19:54:02,382 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 20))
2024-04-20 19:54:02,382 - root - WARNING -validation done ! go ahead......
2024-04-20 19:54:02,383 - root - INFO -reading the file format is parquet 
2024-04-20 19:54:02,383 - Ingest - WARNING -load_file method started!...
2024-04-20 19:54:02,682 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-20 19:54:02,684 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-20 19:54:03,376 - root - INFO -validating the dataframe......
2024-04-20 19:54:03,376 - Ingest - WARNING -count the records df_city 
2024-04-20 19:54:03,573 - root - INFO -total number of records in the dataframe 28338
2024-04-20 19:54:03,573 - Ingest - WARNING -load_file method started!...
2024-04-20 19:54:06,118 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-20 19:54:06,119 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-20 19:54:06,228 - root - INFO -validating the dataframe......
2024-04-20 19:54:06,228 - Ingest - WARNING -count the records df_fact 
2024-04-20 19:54:06,857 - root - INFO -total number of records in the dataframe 1329329
2024-04-20 19:54:06,857 - root - INFO -data processing..........
2024-04-20 19:54:06,857 - root - WARNING -data_clean method() started ......
2024-04-20 19:54:06,857 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-20 19:54:06,884 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-20 19:54:06,895 - root - WARNING -Adding a new column to df_presc_sel
2024-04-20 19:54:06,901 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-20 19:54:06,921 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-20 19:54:06,931 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-20 19:54:06,939 - root - WARNING -Checking the null values in all columns
2024-04-20 19:54:07,015 - root - WARNING -drop the null values in the respective columns.......
2024-04-20 19:54:07,033 - root - WARNING -successfully dropped the null values.....
2024-04-20 19:54:07,033 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-20 19:54:17,175 - root - INFO -validating schema for dataframes.....
2024-04-20 19:54:17,177 - root - INFO -Applications name .......   
2024-04-20 19:54:46,268 - root - INFO -I am the driver applications.........
2024-04-20 19:54:46,268 - root - INFO -Calling the spark object.........  
2024-04-20 19:54:47,734 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x104076ad0>
2024-04-20 19:54:47,734 - root - INFO -validating the spark object
2024-04-20 19:54:49,608 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 20))
2024-04-20 19:54:49,608 - root - WARNING -validation done ! go ahead......
2024-04-20 19:54:49,609 - root - INFO -reading the file format is parquet 
2024-04-20 19:54:49,609 - Ingest - WARNING -load_file method started!...
2024-04-20 19:54:49,864 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-20 19:54:49,866 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-20 19:54:50,236 - root - INFO -validating the dataframe......
2024-04-20 19:54:50,236 - Ingest - WARNING -count the records df_city 
2024-04-20 19:54:50,424 - root - INFO -total number of records in the dataframe 28338
2024-04-20 19:54:50,424 - Ingest - WARNING -load_file method started!...
2024-04-20 19:54:52,969 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-20 19:54:52,971 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-20 19:54:53,088 - root - INFO -validating the dataframe......
2024-04-20 19:54:53,089 - Ingest - WARNING -count the records df_fact 
2024-04-20 19:54:53,500 - root - INFO -total number of records in the dataframe 1329329
2024-04-20 19:54:53,500 - root - INFO -data processing..........
2024-04-20 19:54:53,500 - root - WARNING -data_clean method() started ......
2024-04-20 19:54:53,500 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-20 19:54:53,514 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-20 19:54:53,523 - root - WARNING -Adding a new column to df_presc_sel
2024-04-20 19:54:53,527 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-20 19:54:53,545 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-20 19:54:53,558 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-20 19:54:53,563 - root - WARNING -Checking the null values in all columns
2024-04-20 19:54:53,563 - root - WARNING -drop the null values in the respective columns.......
2024-04-20 19:54:53,574 - root - WARNING -successfully dropped the null values.....
2024-04-20 19:54:53,574 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-20 19:54:53,736 - root - INFO -validating schema for dataframes.....
2024-04-20 19:54:53,738 - root - INFO -Applications name .......   
2024-04-23 23:51:02,208 - root - INFO -I am the driver applications.........
2024-04-23 23:51:02,209 - root - INFO -Calling the spark object.........  
2024-04-23 23:51:04,453 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x1020b20e0>
2024-04-23 23:51:04,454 - root - INFO -validating the spark object
2024-04-23 23:51:06,964 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 23))
2024-04-23 23:51:06,965 - root - WARNING -validation done ! go ahead......
2024-04-23 23:51:06,965 - root - INFO -reading the file format is parquet 
2024-04-23 23:51:06,965 - Ingest - WARNING -load_file method started!...
2024-04-23 23:51:07,313 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-23 23:51:07,316 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-23 23:51:08,046 - root - INFO -validating the dataframe......
2024-04-23 23:51:08,046 - Ingest - WARNING -count the records df_city 
2024-04-23 23:51:08,331 - root - INFO -total number of records in the dataframe 28338
2024-04-23 23:51:08,331 - Ingest - WARNING -load_file method started!...
2024-04-23 23:51:11,638 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-23 23:51:11,640 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-23 23:51:11,799 - root - INFO -validating the dataframe......
2024-04-23 23:51:11,799 - Ingest - WARNING -count the records df_fact 
2024-04-23 23:51:12,183 - root - INFO -total number of records in the dataframe 1329329
2024-04-23 23:51:12,183 - root - INFO -data processing..........
2024-04-23 23:51:12,183 - root - WARNING -data_clean method() started ......
2024-04-23 23:51:12,183 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-23 23:51:12,197 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-23 23:51:12,206 - root - WARNING -Adding a new column to df_presc_sel
2024-04-23 23:51:12,211 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-23 23:51:12,228 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-23 23:51:12,238 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-23 23:51:12,253 - root - WARNING -Checking the null values in all columns
2024-04-23 23:51:12,267 - root - WARNING -successfully dropped the null values.....
2024-04-23 23:51:12,267 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-23 23:51:12,441 - root - INFO -validating schema for dataframes.....
2024-04-23 23:51:12,443 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 00:16:34,841 - root - INFO -I am the driver applications.........
2024-04-24 00:16:34,841 - root - INFO -Calling the spark object.........  
2024-04-24 00:16:36,726 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x10707a8c0>
2024-04-24 00:16:36,726 - root - INFO -validating the spark object
2024-04-24 00:16:38,671 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 00:16:38,671 - root - WARNING -validation done ! go ahead......
2024-04-24 00:16:38,672 - root - INFO -reading the file format is parquet 
2024-04-24 00:16:38,672 - Ingest - WARNING -load_file method started!...
2024-04-24 00:16:38,960 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 00:16:38,963 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 00:16:39,481 - root - INFO -validating the dataframe......
2024-04-24 00:16:39,481 - Ingest - WARNING -count the records df_city 
2024-04-24 00:16:39,740 - root - INFO -total number of records in the dataframe 28338
2024-04-24 00:16:39,740 - Ingest - WARNING -load_file method started!...
2024-04-24 00:16:42,497 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 00:16:42,498 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 00:16:42,625 - root - INFO -validating the dataframe......
2024-04-24 00:16:42,625 - Ingest - WARNING -count the records df_fact 
2024-04-24 00:16:43,019 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 00:16:43,019 - root - INFO -data processing..........
2024-04-24 00:16:43,019 - root - WARNING -data_clean method() started ......
2024-04-24 00:16:43,019 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 00:16:43,043 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 00:16:43,055 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 00:16:43,060 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 00:16:43,076 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 00:16:43,089 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 00:16:43,097 - root - WARNING -Checking the null values in all columns
2024-04-24 00:16:43,114 - root - WARNING -successfully dropped the null values.....
2024-04-24 00:16:43,114 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 00:16:43,313 - root - INFO -validating schema for dataframes.....
2024-04-24 00:16:43,315 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 00:17:46,432 - root - INFO -I am the driver applications.........
2024-04-24 00:17:46,433 - root - INFO -Calling the spark object.........  
2024-04-24 00:17:48,218 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x103f02320>
2024-04-24 00:17:48,218 - root - INFO -validating the spark object
2024-04-24 00:17:50,153 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 00:17:50,153 - root - WARNING -validation done ! go ahead......
2024-04-24 00:17:50,154 - root - INFO -reading the file format is parquet 
2024-04-24 00:17:50,154 - Ingest - WARNING -load_file method started!...
2024-04-24 00:17:50,487 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 00:17:50,491 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 00:17:50,978 - root - INFO -validating the dataframe......
2024-04-24 00:17:50,978 - Ingest - WARNING -count the records df_city 
2024-04-24 00:17:51,165 - root - INFO -total number of records in the dataframe 28338
2024-04-24 00:17:51,165 - Ingest - WARNING -load_file method started!...
2024-04-24 00:17:53,881 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 00:17:53,883 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 00:17:53,998 - root - INFO -validating the dataframe......
2024-04-24 00:17:53,998 - Ingest - WARNING -count the records df_fact 
2024-04-24 00:17:54,384 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 00:17:54,384 - root - INFO -data processing..........
2024-04-24 00:17:54,385 - root - WARNING -data_clean method() started ......
2024-04-24 00:17:54,385 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 00:18:14,106 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 00:18:14,118 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 00:18:14,125 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 00:18:14,145 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 00:18:14,157 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 00:18:14,163 - root - WARNING -Checking the null values in all columns
2024-04-24 00:18:14,183 - root - WARNING -successfully dropped the null values.....
2024-04-24 00:18:14,183 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 00:18:23,474 - root - INFO -validating schema for dataframes.....
2024-04-24 00:18:23,474 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 00:19:09,474 - root - INFO -I am the driver applications.........
2024-04-24 00:19:09,475 - root - INFO -Calling the spark object.........  
2024-04-24 00:19:11,264 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x1073aa3b0>
2024-04-24 00:19:11,264 - root - INFO -validating the spark object
2024-04-24 00:19:13,310 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 00:19:13,310 - root - WARNING -validation done ! go ahead......
2024-04-24 00:19:13,311 - root - INFO -reading the file format is parquet 
2024-04-24 00:19:13,311 - Ingest - WARNING -load_file method started!...
2024-04-24 00:19:13,601 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 00:19:13,604 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 00:19:14,004 - root - INFO -validating the dataframe......
2024-04-24 00:19:14,004 - Ingest - WARNING -count the records df_city 
2024-04-24 00:19:14,201 - root - INFO -total number of records in the dataframe 28338
2024-04-24 00:19:14,202 - Ingest - WARNING -load_file method started!...
2024-04-24 00:19:16,741 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 00:19:16,743 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 00:19:16,857 - root - INFO -validating the dataframe......
2024-04-24 00:19:16,857 - Ingest - WARNING -count the records df_fact 
2024-04-24 00:19:17,245 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 00:19:17,245 - root - INFO -data processing..........
2024-04-24 00:19:17,245 - root - WARNING -data_clean method() started ......
2024-04-24 00:19:17,245 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 00:19:17,263 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 00:19:17,274 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 00:19:17,279 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 00:19:17,298 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 00:19:17,308 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 00:19:17,313 - root - WARNING -Checking the null values in all columns
2024-04-24 00:19:17,329 - root - WARNING -successfully dropped the null values.....
2024-04-24 00:19:17,329 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 00:19:22,968 - root - INFO -validating schema for dataframes.....
2024-04-24 00:19:22,969 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 00:19:51,913 - root - INFO -I am the driver applications.........
2024-04-24 00:19:51,913 - root - INFO -Calling the spark object.........  
2024-04-24 00:19:53,709 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x104eaa380>
2024-04-24 00:19:53,710 - root - INFO -validating the spark object
2024-04-24 00:19:55,536 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 00:19:55,537 - root - WARNING -validation done ! go ahead......
2024-04-24 00:19:55,537 - root - INFO -reading the file format is parquet 
2024-04-24 00:19:55,537 - Ingest - WARNING -load_file method started!...
2024-04-24 00:19:55,803 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 00:19:55,805 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 00:19:56,187 - root - INFO -validating the dataframe......
2024-04-24 00:19:56,187 - Ingest - WARNING -count the records df_city 
2024-04-24 00:19:56,390 - root - INFO -total number of records in the dataframe 28338
2024-04-24 00:19:56,390 - Ingest - WARNING -load_file method started!...
2024-04-24 00:19:58,962 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 00:19:58,964 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 00:19:59,078 - root - INFO -validating the dataframe......
2024-04-24 00:19:59,078 - Ingest - WARNING -count the records df_fact 
2024-04-24 00:19:59,507 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 00:19:59,508 - root - INFO -data processing..........
2024-04-24 00:19:59,508 - root - WARNING -data_clean method() started ......
2024-04-24 00:19:59,508 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 00:19:59,523 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 00:19:59,536 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 00:19:59,541 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 00:19:59,560 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 00:19:59,573 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 00:19:59,588 - root - WARNING -Checking the null values in all columns
2024-04-24 00:19:59,615 - root - WARNING -successfully dropped the null values.....
2024-04-24 00:19:59,616 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 00:20:02,906 - root - INFO -validating schema for dataframes.....
2024-04-24 00:20:02,907 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 00:20:02,907 - root - INFO -data_transformation executing....
2024-04-24 00:20:02,908 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-24 00:20:02,910 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, count_name: string, population: int, zips: string] 
2024-04-24 00:20:04,035 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-24 00:20:04,893 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-24 00:22:48,665 - root - INFO -I am the driver applications.........
2024-04-24 00:22:48,666 - root - INFO -Calling the spark object.........  
2024-04-24 00:22:50,258 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x10236f430>
2024-04-24 00:22:50,258 - root - INFO -validating the spark object
2024-04-24 00:22:52,064 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 00:22:52,064 - root - WARNING -validation done ! go ahead......
2024-04-24 00:22:52,064 - root - INFO -reading the file format is parquet 
2024-04-24 00:22:52,064 - Ingest - WARNING -load_file method started!...
2024-04-24 00:22:52,359 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 00:22:52,361 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 00:22:52,723 - root - INFO -validating the dataframe......
2024-04-24 00:22:52,723 - Ingest - WARNING -count the records df_city 
2024-04-24 00:22:52,911 - root - INFO -total number of records in the dataframe 28338
2024-04-24 00:22:52,911 - Ingest - WARNING -load_file method started!...
2024-04-24 00:22:55,658 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 00:22:55,659 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 00:22:55,780 - root - INFO -validating the dataframe......
2024-04-24 00:22:55,780 - Ingest - WARNING -count the records df_fact 
2024-04-24 00:22:56,192 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 00:22:56,192 - root - INFO -data processing..........
2024-04-24 00:22:56,192 - root - WARNING -data_clean method() started ......
2024-04-24 00:22:56,192 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 00:22:56,207 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 00:22:56,218 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 00:22:56,223 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 00:22:56,238 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 00:22:56,248 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 00:22:56,256 - root - WARNING -Checking the null values in all columns
2024-04-24 00:22:56,274 - root - WARNING -successfully dropped the null values.....
2024-04-24 00:22:56,274 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 00:22:59,080 - root - INFO -validating schema for dataframes.....
2024-04-24 00:22:59,081 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 00:22:59,081 - root - INFO -data_transformation executing....
2024-04-24 00:22:59,081 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-24 00:22:59,082 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, count_name: string, population: int, zips: string] 
2024-04-24 00:22:59,927 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-24 00:23:00,575 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-24 00:24:40,440 - root - INFO -I am the driver applications.........
2024-04-24 00:24:40,440 - root - INFO -Calling the spark object.........  
2024-04-24 00:24:42,083 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x10465f490>
2024-04-24 00:24:42,083 - root - INFO -validating the spark object
2024-04-24 00:24:43,879 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 00:24:43,880 - root - WARNING -validation done ! go ahead......
2024-04-24 00:24:43,880 - root - INFO -reading the file format is parquet 
2024-04-24 00:24:43,880 - Ingest - WARNING -load_file method started!...
2024-04-24 00:24:44,128 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 00:24:44,130 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 00:24:44,513 - root - INFO -validating the dataframe......
2024-04-24 00:24:44,513 - Ingest - WARNING -count the records df_city 
2024-04-24 00:24:44,702 - root - INFO -total number of records in the dataframe 28338
2024-04-24 00:24:44,702 - Ingest - WARNING -load_file method started!...
2024-04-24 00:24:47,292 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 00:24:47,293 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 00:24:47,404 - root - INFO -validating the dataframe......
2024-04-24 00:24:47,404 - Ingest - WARNING -count the records df_fact 
2024-04-24 00:24:47,799 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 00:24:47,799 - root - INFO -data processing..........
2024-04-24 00:24:47,799 - root - WARNING -data_clean method() started ......
2024-04-24 00:24:47,799 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 00:24:47,814 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 00:24:47,825 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 00:24:47,830 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 00:24:47,845 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 00:24:47,856 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 00:24:47,863 - root - WARNING -Checking the null values in all columns
2024-04-24 00:24:47,882 - root - WARNING -successfully dropped the null values.....
2024-04-24 00:24:47,882 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 00:24:50,321 - root - INFO -validating schema for dataframes.....
2024-04-24 00:24:50,322 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 00:24:50,322 - root - INFO -data_transformation executing....
2024-04-24 00:24:50,322 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-24 00:24:50,323 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, count_name: string, population: int, zips: string] 
2024-04-24 00:24:51,149 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-24 00:24:51,943 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-24 00:25:56,091 - root - INFO -I am the driver applications.........
2024-04-24 00:25:56,091 - root - INFO -Calling the spark object.........  
2024-04-24 00:25:57,824 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x1072ab430>
2024-04-24 00:25:57,824 - root - INFO -validating the spark object
2024-04-24 00:25:59,817 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 00:25:59,817 - root - WARNING -validation done ! go ahead......
2024-04-24 00:25:59,817 - root - INFO -reading the file format is parquet 
2024-04-24 00:25:59,817 - Ingest - WARNING -load_file method started!...
2024-04-24 00:26:00,096 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 00:26:00,099 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 00:26:00,517 - root - INFO -validating the dataframe......
2024-04-24 00:26:00,517 - Ingest - WARNING -count the records df_city 
2024-04-24 00:26:00,761 - root - INFO -total number of records in the dataframe 28338
2024-04-24 00:26:00,762 - Ingest - WARNING -load_file method started!...
2024-04-24 00:26:03,443 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 00:26:03,445 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 00:26:03,557 - root - INFO -validating the dataframe......
2024-04-24 00:26:03,557 - Ingest - WARNING -count the records df_fact 
2024-04-24 00:26:03,995 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 00:26:03,996 - root - INFO -data processing..........
2024-04-24 00:26:03,996 - root - WARNING -data_clean method() started ......
2024-04-24 00:26:03,996 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 00:26:04,011 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 00:26:04,023 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 00:26:04,028 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 00:26:04,046 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 00:26:04,062 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 00:26:04,069 - root - WARNING -Checking the null values in all columns
2024-04-24 00:26:04,085 - root - WARNING -successfully dropped the null values.....
2024-04-24 00:26:04,086 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 00:26:07,265 - root - INFO -validating schema for dataframes.....
2024-04-24 00:26:07,265 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 00:26:07,265 - root - INFO -data_transformation executing....
2024-04-24 00:26:07,267 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-24 00:26:07,268 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, count_name: string, population: int, zips: string] 
2024-04-24 00:26:08,294 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-24 00:26:09,087 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-24 00:26:13,522 - Data_transformation - WARNING -Data_report1 successfully executed .... , go forward
2024-04-24 00:26:13,523 - root - INFO -displaying the df_report_1
2024-04-24 00:26:17,431 - root - INFO -Applications name .......   
2024-04-24 00:28:03,403 - root - INFO -I am the driver applications.........
2024-04-24 00:28:03,404 - root - INFO -Calling the spark object.........  
2024-04-24 00:28:05,220 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x1034a9330>
2024-04-24 00:28:05,220 - root - INFO -validating the spark object
2024-04-24 00:28:07,204 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 00:28:07,204 - root - WARNING -validation done ! go ahead......
2024-04-24 00:28:07,204 - root - INFO -reading the file format is parquet 
2024-04-24 00:28:07,204 - Ingest - WARNING -load_file method started!...
2024-04-24 00:28:07,489 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 00:28:07,491 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 00:28:07,859 - root - INFO -validating the dataframe......
2024-04-24 00:28:07,859 - Ingest - WARNING -count the records df_city 
2024-04-24 00:28:08,053 - root - INFO -total number of records in the dataframe 28338
2024-04-24 00:28:08,054 - Ingest - WARNING -load_file method started!...
2024-04-24 00:28:10,842 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 00:28:10,844 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 00:28:10,961 - root - INFO -validating the dataframe......
2024-04-24 00:28:10,961 - Ingest - WARNING -count the records df_fact 
2024-04-24 00:28:11,394 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 00:28:11,394 - root - INFO -data processing..........
2024-04-24 00:28:11,394 - root - WARNING -data_clean method() started ......
2024-04-24 00:28:11,394 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 00:28:11,409 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 00:28:11,418 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 00:28:11,423 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 00:28:11,439 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 00:28:11,452 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 00:28:11,459 - root - WARNING -Checking the null values in all columns
2024-04-24 00:28:11,476 - root - WARNING -successfully dropped the null values.....
2024-04-24 00:28:11,476 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 00:28:11,660 - root - INFO -validating schema for dataframes.....
2024-04-24 00:28:11,661 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 00:28:11,661 - root - INFO -data_transformation executing....
2024-04-24 00:28:11,661 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-24 00:28:11,662 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string] 
2024-04-24 00:28:11,682 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-24 00:28:11,699 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-24 00:28:11,722 - Data_transformation - WARNING -Data_report1 successfully executed .... , go forward
2024-04-24 00:28:11,722 - root - INFO -displaying the df_report_1
2024-04-24 00:28:15,235 - root - INFO -Applications name .......   
2024-04-24 19:35:36,509 - root - INFO -I am the driver applications.........
2024-04-24 19:35:36,510 - root - INFO -Calling the spark object.........  
2024-04-24 19:35:38,690 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x10352ead0>
2024-04-24 19:35:38,690 - root - INFO -validating the spark object
2024-04-24 19:35:40,781 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 19:35:40,781 - root - WARNING -validation done ! go ahead......
2024-04-24 19:35:40,782 - root - INFO -reading the file format is parquet 
2024-04-24 19:35:40,782 - Ingest - WARNING -load_file method started!...
2024-04-24 19:35:41,062 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 19:35:41,064 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 19:35:41,863 - root - INFO -validating the dataframe......
2024-04-24 19:35:41,863 - Ingest - WARNING -count the records df_city 
2024-04-24 19:35:42,097 - root - INFO -total number of records in the dataframe 28338
2024-04-24 19:35:42,098 - Ingest - WARNING -load_file method started!...
2024-04-24 19:35:44,841 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 19:35:44,843 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 19:35:44,971 - root - INFO -validating the dataframe......
2024-04-24 19:35:44,971 - Ingest - WARNING -count the records df_fact 
2024-04-24 19:35:45,564 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 19:35:45,564 - root - INFO -data processing..........
2024-04-24 19:35:45,564 - root - WARNING -data_clean method() started ......
2024-04-24 19:35:45,564 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 19:35:45,584 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 19:35:45,594 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 19:35:45,602 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 19:35:45,628 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 19:35:45,643 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 19:35:45,649 - root - WARNING -Checking the null values in all columns
2024-04-24 19:35:45,665 - root - WARNING -successfully dropped the null values.....
2024-04-24 19:35:45,665 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 19:35:45,856 - root - INFO -validating schema for dataframes.....
2024-04-24 19:35:45,857 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 19:35:45,857 - root - INFO -data_transformation executing....
2024-04-24 19:35:45,857 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-24 19:35:45,858 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string] 
2024-04-24 19:35:45,874 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-24 19:35:45,889 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-24 19:35:45,912 - Data_transformation - WARNING -Data_report1 successfully executed .... , go forward
2024-04-24 19:35:45,912 - root - INFO -displaying the df_report_1
2024-04-24 19:35:45,912 - root - INFO -displaying data_report2 method.... 
2024-04-24 19:35:45,912 - Data_transformation - WARNING -executing data_report2 method ...
2024-04-24 19:35:45,912 - Data_transformation - WARNING -executing the task ::: consider the prescribers only from 20 to 50 years_of_exp and rank the prescribers based on their tx_cnt for each state
2024-04-24 19:36:31,018 - root - INFO -I am the driver applications.........
2024-04-24 19:36:31,018 - root - INFO -Calling the spark object.........  
2024-04-24 19:36:32,615 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x106872ad0>
2024-04-24 19:36:32,615 - root - INFO -validating the spark object
2024-04-24 19:36:34,406 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 19:36:34,406 - root - WARNING -validation done ! go ahead......
2024-04-24 19:36:34,407 - root - INFO -reading the file format is parquet 
2024-04-24 19:36:34,407 - Ingest - WARNING -load_file method started!...
2024-04-24 19:36:34,687 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 19:36:34,689 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 19:36:35,086 - root - INFO -validating the dataframe......
2024-04-24 19:36:35,086 - Ingest - WARNING -count the records df_city 
2024-04-24 19:36:35,277 - root - INFO -total number of records in the dataframe 28338
2024-04-24 19:36:35,277 - Ingest - WARNING -load_file method started!...
2024-04-24 19:36:37,762 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 19:36:37,764 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 19:36:37,879 - root - INFO -validating the dataframe......
2024-04-24 19:36:37,879 - Ingest - WARNING -count the records df_fact 
2024-04-24 19:36:38,234 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 19:36:38,234 - root - INFO -data processing..........
2024-04-24 19:36:38,234 - root - WARNING -data_clean method() started ......
2024-04-24 19:36:38,234 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 19:36:38,247 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 19:36:38,258 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 19:36:38,265 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 19:36:38,281 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 19:36:38,293 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 19:36:38,300 - root - WARNING -Checking the null values in all columns
2024-04-24 19:36:38,315 - root - WARNING -successfully dropped the null values.....
2024-04-24 19:36:38,315 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 19:36:38,506 - root - INFO -validating schema for dataframes.....
2024-04-24 19:36:38,507 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 19:36:38,507 - root - INFO -data_transformation executing....
2024-04-24 19:36:38,508 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-24 19:36:38,508 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string] 
2024-04-24 19:36:38,526 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-24 19:36:38,544 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-24 19:36:38,568 - Data_transformation - WARNING -Data_report1 successfully executed .... , go forward
2024-04-24 19:36:38,568 - root - INFO -displaying the df_report_1
2024-04-24 19:36:38,568 - root - INFO -displaying data_report2 method.... 
2024-04-24 19:36:38,568 - Data_transformation - WARNING -executing data_report2 method ...
2024-04-24 19:36:38,568 - Data_transformation - WARNING -executing the task ::: consider the prescribers only from 20 to 50 years_of_exp and rank the prescribers based on their tx_cnt for each state
2024-04-24 19:38:01,478 - root - INFO -I am the driver applications.........
2024-04-24 19:38:01,479 - root - INFO -Calling the spark object.........  
2024-04-24 19:38:03,071 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x103ba6ad0>
2024-04-24 19:38:03,071 - root - INFO -validating the spark object
2024-04-24 19:38:04,932 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 19:38:04,932 - root - WARNING -validation done ! go ahead......
2024-04-24 19:38:04,932 - root - INFO -reading the file format is parquet 
2024-04-24 19:38:04,932 - Ingest - WARNING -load_file method started!...
2024-04-24 19:38:05,303 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 19:38:05,307 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 19:38:05,751 - root - INFO -validating the dataframe......
2024-04-24 19:38:05,751 - Ingest - WARNING -count the records df_city 
2024-04-24 19:38:05,944 - root - INFO -total number of records in the dataframe 28338
2024-04-24 19:38:05,944 - Ingest - WARNING -load_file method started!...
2024-04-24 19:38:08,519 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 19:38:08,521 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 19:38:08,649 - root - INFO -validating the dataframe......
2024-04-24 19:38:08,649 - Ingest - WARNING -count the records df_fact 
2024-04-24 19:38:09,058 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 19:38:09,058 - root - INFO -data processing..........
2024-04-24 19:38:09,058 - root - WARNING -data_clean method() started ......
2024-04-24 19:38:09,058 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 19:38:09,074 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 19:38:09,082 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 19:38:09,087 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 19:38:09,102 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 19:38:09,111 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 19:38:09,119 - root - WARNING -Checking the null values in all columns
2024-04-24 19:38:09,129 - root - WARNING -successfully dropped the null values.....
2024-04-24 19:38:09,129 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 19:38:09,290 - root - INFO -validating schema for dataframes.....
2024-04-24 19:38:09,291 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 19:38:09,292 - root - INFO -data_transformation executing....
2024-04-24 19:38:09,292 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-24 19:38:09,292 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string] 
2024-04-24 19:38:09,309 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-24 19:38:09,327 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-24 19:38:09,355 - Data_transformation - WARNING -Data_report1 successfully executed .... , go forward
2024-04-24 19:38:09,355 - root - INFO -displaying the df_report_1
2024-04-24 19:38:09,355 - root - INFO -displaying data_report2 method.... 
2024-04-24 19:38:09,355 - Data_transformation - WARNING -executing data_report2 method ...
2024-04-24 19:38:09,355 - Data_transformation - WARNING -executing the task ::: consider the prescribers only from 20 to 50 Years_of_exp and rank the prescribers based on their tx_cnt for each state
2024-04-24 19:38:09,396 - Data_transformation - WARNING -data_report2 method executed...., go frwd... 
2024-04-24 19:38:13,689 - root - INFO -extracting files to Output .....
2024-04-24 19:42:03,941 - root - INFO -I am the driver applications.........
2024-04-24 19:42:03,946 - root - INFO -Calling the spark object.........  
2024-04-24 19:42:05,574 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x104971b10>
2024-04-24 19:42:05,574 - root - INFO -validating the spark object
2024-04-24 19:42:07,418 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 19:42:07,418 - root - WARNING -validation done ! go ahead......
2024-04-24 19:42:07,419 - root - INFO -reading the file format is parquet 
2024-04-24 19:42:07,419 - Ingest - WARNING -load_file method started!...
2024-04-24 19:42:07,719 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 19:42:07,721 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 19:42:08,628 - root - INFO -validating the dataframe......
2024-04-24 19:42:08,628 - Ingest - WARNING -count the records df_city 
2024-04-24 19:42:08,905 - root - INFO -total number of records in the dataframe 28338
2024-04-24 19:42:08,906 - Ingest - WARNING -load_file method started!...
2024-04-24 19:42:11,620 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 19:42:11,622 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 19:42:11,730 - root - INFO -validating the dataframe......
2024-04-24 19:42:11,730 - Ingest - WARNING -count the records df_fact 
2024-04-24 19:42:12,093 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 19:42:12,093 - root - INFO -data processing..........
2024-04-24 19:42:12,093 - root - WARNING -data_clean method() started ......
2024-04-24 19:42:12,093 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 19:42:12,107 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 19:42:12,117 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 19:42:12,122 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 19:42:12,137 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 19:42:12,147 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 19:42:12,153 - root - WARNING -Checking the null values in all columns
2024-04-24 19:42:12,169 - root - WARNING -successfully dropped the null values.....
2024-04-24 19:42:12,169 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 19:42:12,339 - root - INFO -validating schema for dataframes.....
2024-04-24 19:42:12,340 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 19:42:12,340 - root - INFO -data_transformation executing....
2024-04-24 19:42:12,340 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-24 19:42:12,340 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string] 
2024-04-24 19:42:12,357 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-24 19:42:12,376 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-24 19:42:12,401 - Data_transformation - WARNING -Data_report1 successfully executed .... , go forward
2024-04-24 19:42:12,401 - root - INFO -displaying the df_report_1
2024-04-24 19:42:12,401 - root - INFO -displaying data_report2 method.... 
2024-04-24 19:42:12,401 - Data_transformation - WARNING -executing data_report2 method ...
2024-04-24 19:42:12,401 - Data_transformation - WARNING -executing the task ::: consider the prescribers only from 20 to 50 Years_of_exp and rank the prescribers based on their tx_cnt for each state
2024-04-24 19:42:12,446 - Data_transformation - WARNING -data_report2 method executed...., go frwd... 
2024-04-24 19:42:17,081 - root - INFO -extracting files to Output .....
2024-04-24 19:42:17,082 - root - INFO -total amount of time taken via process : 13.14 seconds
2024-04-24 19:42:17,082 - root - INFO -Applications name .......   
2024-04-24 23:44:08,010 - root - INFO -I am the driver applications.........
2024-04-24 23:44:08,010 - root - INFO -Calling the spark object.........  
2024-04-24 23:44:10,471 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x10336ceb0>
2024-04-24 23:44:10,471 - root - INFO -validating the spark object
2024-04-24 23:44:12,506 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 23:44:12,506 - root - WARNING -validation done ! go ahead......
2024-04-24 23:44:12,506 - root - INFO -reading the file format is parquet 
2024-04-24 23:44:12,506 - Ingest - WARNING -load_file method started!...
2024-04-24 23:44:12,827 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 23:44:12,829 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 23:44:13,254 - root - INFO -validating the dataframe......
2024-04-24 23:44:13,254 - Ingest - WARNING -count the records df_city 
2024-04-24 23:44:13,449 - root - INFO -total number of records in the dataframe 28338
2024-04-24 23:44:13,449 - Ingest - WARNING -load_file method started!...
2024-04-24 23:44:15,988 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 23:44:15,990 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 23:44:16,121 - root - INFO -validating the dataframe......
2024-04-24 23:44:16,121 - Ingest - WARNING -count the records df_fact 
2024-04-24 23:44:16,565 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 23:44:16,565 - root - INFO -data processing..........
2024-04-24 23:44:16,565 - root - WARNING -data_clean method() started ......
2024-04-24 23:44:16,565 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 23:44:16,585 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 23:44:16,596 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 23:44:16,610 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 23:44:16,629 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 23:44:16,644 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 23:44:16,650 - root - WARNING -Checking the null values in all columns
2024-04-24 23:44:16,674 - root - WARNING -successfully dropped the null values.....
2024-04-24 23:44:16,674 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 23:44:16,899 - root - INFO -validating schema for dataframes.....
2024-04-24 23:44:16,900 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 23:44:16,900 - root - INFO -data_transformation executing....
2024-04-24 23:44:16,900 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-24 23:44:16,901 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string] 
2024-04-24 23:44:16,918 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-24 23:44:16,938 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-24 23:44:16,959 - Data_transformation - WARNING -Data_report1 successfully executed .... , go forward
2024-04-24 23:44:16,959 - root - INFO -displaying the df_report_1
2024-04-24 23:44:16,959 - root - INFO -displaying data_report2 method.... 
2024-04-24 23:44:16,959 - Data_transformation - WARNING -executing data_report2 method ...
2024-04-24 23:44:16,959 - Data_transformation - WARNING -executing the task ::: consider the prescribers only from 20 to 50 Years_of_exp and rank the prescribers based on their tx_cnt for each state
2024-04-24 23:44:16,998 - Data_transformation - WARNING -data_report2 method executed...., go frwd... 
2024-04-24 23:44:21,462 - root - INFO -extracting files to Output .....
2024-04-24 23:44:21,462 - Extraction - WARNING -extract_files method started executing ....
2024-04-24 23:45:11,190 - root - INFO -I am the driver applications.........
2024-04-24 23:45:11,191 - root - INFO -Calling the spark object.........  
2024-04-24 23:45:12,778 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x103d6cb80>
2024-04-24 23:45:12,778 - root - INFO -validating the spark object
2024-04-24 23:45:14,627 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 24))
2024-04-24 23:45:14,627 - root - WARNING -validation done ! go ahead......
2024-04-24 23:45:14,627 - root - INFO -reading the file format is parquet 
2024-04-24 23:45:14,628 - Ingest - WARNING -load_file method started!...
2024-04-24 23:45:14,889 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-24 23:45:14,891 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-24 23:45:15,273 - root - INFO -validating the dataframe......
2024-04-24 23:45:15,273 - Ingest - WARNING -count the records df_city 
2024-04-24 23:45:15,479 - root - INFO -total number of records in the dataframe 28338
2024-04-24 23:45:15,479 - Ingest - WARNING -load_file method started!...
2024-04-24 23:45:18,048 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-24 23:45:18,050 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-24 23:45:18,180 - root - INFO -validating the dataframe......
2024-04-24 23:45:18,180 - Ingest - WARNING -count the records df_fact 
2024-04-24 23:45:18,582 - root - INFO -total number of records in the dataframe 1329329
2024-04-24 23:45:18,582 - root - INFO -data processing..........
2024-04-24 23:45:18,582 - root - WARNING -data_clean method() started ......
2024-04-24 23:45:18,582 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-24 23:45:18,607 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-24 23:45:18,630 - root - WARNING -Adding a new column to df_presc_sel
2024-04-24 23:45:18,637 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-24 23:45:18,654 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-24 23:45:18,666 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-24 23:45:18,673 - root - WARNING -Checking the null values in all columns
2024-04-24 23:45:18,692 - root - WARNING -successfully dropped the null values.....
2024-04-24 23:45:18,692 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-24 23:45:18,859 - root - INFO -validating schema for dataframes.....
2024-04-24 23:45:18,860 - root - INFO -checking for null values in dataframes... after processing 
2024-04-24 23:45:18,860 - root - INFO -data_transformation executing....
2024-04-24 23:45:18,860 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-24 23:45:18,861 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string] 
2024-04-24 23:45:18,878 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-24 23:45:18,899 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-24 23:45:18,927 - Data_transformation - WARNING -Data_report1 successfully executed .... , go forward
2024-04-24 23:45:18,927 - root - INFO -displaying the df_report_1
2024-04-24 23:45:18,927 - root - INFO -displaying data_report2 method.... 
2024-04-24 23:45:18,927 - Data_transformation - WARNING -executing data_report2 method ...
2024-04-24 23:45:18,927 - Data_transformation - WARNING -executing the task ::: consider the prescribers only from 20 to 50 Years_of_exp and rank the prescribers based on their tx_cnt for each state
2024-04-24 23:45:18,967 - Data_transformation - WARNING -data_report2 method executed...., go frwd... 
2024-04-24 23:45:23,689 - root - INFO -extracting files to Output .....
2024-04-24 23:45:23,689 - Extraction - WARNING -extract_files method started executing ....
2024-04-24 23:45:27,379 - Extraction - WARNING -extract_file method successfully executed.....
2024-04-24 23:45:27,379 - Extraction - WARNING -extract_files method started executing ....
2024-04-24 23:45:32,354 - Extraction - WARNING -extract_file method successfully executed.....
2024-04-24 23:45:32,354 - root - INFO -extraction files to output completed..... 
2024-04-24 23:45:32,354 - root - INFO -total amount of time taken via process : 21.16 seconds
2024-04-24 23:45:32,354 - root - INFO -Applications name .......   
2024-04-25 00:30:48,707 - root - INFO -I am the driver applications.........
2024-04-25 00:30:48,708 - root - INFO -Calling the spark object.........  
2024-04-25 00:30:50,922 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x1055af580>
2024-04-25 00:30:50,922 - root - INFO -validating the spark object
2024-04-25 00:30:52,917 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 25))
2024-04-25 00:30:52,918 - root - WARNING -validation done ! go ahead......
2024-04-25 00:30:52,918 - root - INFO -reading the file format is parquet 
2024-04-25 00:30:52,918 - Ingest - WARNING -load_file method started!...
2024-04-25 00:30:53,231 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-25 00:30:53,233 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-25 00:30:53,620 - root - INFO -validating the dataframe......
2024-04-25 00:30:53,620 - Ingest - WARNING -count the records df_city 
2024-04-25 00:30:53,808 - root - INFO -total number of records in the dataframe 28338
2024-04-25 00:30:53,808 - Ingest - WARNING -load_file method started!...
2024-04-25 00:30:56,418 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-25 00:30:56,419 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-25 00:30:56,552 - root - INFO -validating the dataframe......
2024-04-25 00:30:56,552 - Ingest - WARNING -count the records df_fact 
2024-04-25 00:30:56,912 - root - INFO -total number of records in the dataframe 1329329
2024-04-25 00:30:56,913 - root - INFO -data processing..........
2024-04-25 00:30:56,913 - root - WARNING -data_clean method() started ......
2024-04-25 00:30:56,913 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-25 00:30:56,926 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-25 00:30:56,934 - root - WARNING -Adding a new column to df_presc_sel
2024-04-25 00:30:56,939 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-25 00:30:56,953 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-25 00:30:56,966 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-25 00:30:56,973 - root - WARNING -Checking the null values in all columns
2024-04-25 00:30:56,985 - root - WARNING -successfully dropped the null values.....
2024-04-25 00:30:56,986 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-25 00:30:57,175 - root - INFO -validating schema for dataframes.....
2024-04-25 00:30:57,176 - root - INFO -checking for null values in dataframes... after processing 
2024-04-25 00:30:57,176 - root - INFO -data_transformation executing....
2024-04-25 00:30:57,176 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-25 00:30:57,177 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string] 
2024-04-25 00:30:57,194 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-25 00:30:57,211 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-25 00:30:57,234 - Data_transformation - WARNING -Data_report1 successfully executed .... , go forward
2024-04-25 00:30:57,234 - root - INFO -displaying the df_report_1
2024-04-25 00:30:57,234 - root - INFO -displaying data_report2 method.... 
2024-04-25 00:30:57,234 - Data_transformation - WARNING -executing data_report2 method ...
2024-04-25 00:30:57,234 - Data_transformation - WARNING -executing the task ::: consider the prescribers only from 20 to 50 Years_of_exp and rank the prescribers based on their tx_cnt for each state
2024-04-25 00:30:57,276 - Data_transformation - WARNING -data_report2 method executed...., go frwd... 
2024-04-25 00:31:01,849 - root - INFO -extracting files to Output .....
2024-04-25 00:31:01,849 - Extraction - WARNING -extract_files method started executing ....
2024-04-25 00:31:05,711 - Extraction - WARNING -extract_file method successfully executed.....
2024-04-25 00:31:05,712 - Extraction - WARNING -extract_files method started executing ....
2024-04-25 00:31:10,856 - Extraction - WARNING -extract_file method successfully executed.....
2024-04-25 00:31:10,856 - root - INFO -extraction files to output completed..... 
2024-04-25 00:31:10,856 - Persist - WARNING -persisting the data into Hive Table for df_city 
2024-04-25 00:31:10,856 - Persist - WARNING -lets create a database...
2024-04-25 00:31:12,948 - Persist - WARNING -No writing DataFrame[city: string, state_name: string, county_name: string, population: int, zipcounts: int, presc_count: bigint] into hive_table by state_name 
2024-04-25 00:31:17,715 - Persist - WARNING -Data successfully persisted into hive table
2024-04-25 00:31:17,715 - Persist - WARNING -persisting the data into Hive Table for df_presc 
2024-04-25 00:31:17,715 - Persist - WARNING -lets create a database...
2024-04-25 00:31:17,746 - Persist - WARNING -No writing DataFrame[presc_id: int, presc_fullname: string, presc_state: string, Country_name: string, Years_of_exp: string, tx_cnt: int, total_day_supply: int, total_drug_cost: double, dense_rank: int] into hive_table by presc_state 
2024-04-25 00:31:23,741 - Persist - WARNING -Data successfully persisted into hive table
2024-04-25 00:31:23,741 - root - INFO -successfully written into hive
2024-04-25 00:31:23,742 - root - INFO -Now write DataFrame[city: string, state_name: string, county_name: string, population: int, zipcounts: int, presc_count: bigint] into MYSQL 
2024-04-25 00:31:23,742 - root - INFO -total amount of time taken via process : 35.04 seconds
2024-04-25 00:31:23,742 - root - INFO -Applications name .......   
2024-04-26 13:04:52,335 - root - INFO -I am the driver applications.........
2024-04-26 13:04:52,335 - root - INFO -Calling the spark object.........  
2024-04-26 13:04:54,339 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x10363e980>
2024-04-26 13:04:54,339 - root - INFO -validating the spark object
2024-04-26 13:04:56,422 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 26))
2024-04-26 13:04:56,422 - root - WARNING -validation done ! go ahead......
2024-04-26 13:04:56,422 - root - INFO -reading the file format is parquet 
2024-04-26 13:04:56,422 - Ingest - WARNING -load_file method started!...
2024-04-26 13:04:56,738 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-26 13:04:56,740 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-26 13:04:57,159 - root - INFO -validating the dataframe......
2024-04-26 13:04:57,159 - Ingest - WARNING -count the records df_city 
2024-04-26 13:04:57,383 - root - INFO -total number of records in the dataframe 28338
2024-04-26 13:04:57,383 - Ingest - WARNING -load_file method started!...
2024-04-26 13:05:00,154 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-26 13:05:00,156 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-26 13:05:00,286 - root - INFO -validating the dataframe......
2024-04-26 13:05:00,286 - Ingest - WARNING -count the records df_fact 
2024-04-26 13:05:00,685 - root - INFO -total number of records in the dataframe 1329329
2024-04-26 13:05:00,685 - root - INFO -data processing..........
2024-04-26 13:05:00,685 - root - WARNING -data_clean method() started ......
2024-04-26 13:05:00,685 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-26 13:05:00,701 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-26 13:05:00,710 - root - WARNING -Adding a new column to df_presc_sel
2024-04-26 13:05:00,715 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-26 13:05:00,732 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-26 13:05:00,744 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-26 13:05:00,751 - root - WARNING -Checking the null values in all columns
2024-04-26 13:05:00,766 - root - WARNING -successfully dropped the null values.....
2024-04-26 13:05:00,766 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-26 13:05:00,940 - root - INFO -validating schema for dataframes.....
2024-04-26 13:05:00,942 - root - INFO -checking for null values in dataframes... after processing 
2024-04-26 13:05:00,942 - root - INFO -data_transformation executing....
2024-04-26 13:05:00,942 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-26 13:05:00,943 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string] 
2024-04-26 13:05:00,968 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-26 13:05:00,994 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-26 13:05:01,020 - Data_transformation - WARNING -Data_report1 successfully executed .... , go forward
2024-04-26 13:05:01,020 - root - INFO -displaying the df_report_1
2024-04-26 13:05:01,020 - root - INFO -displaying data_report2 method.... 
2024-04-26 13:05:01,020 - Data_transformation - WARNING -executing data_report2 method ...
2024-04-26 13:05:01,020 - Data_transformation - WARNING -executing the task ::: consider the prescribers only from 20 to 50 Years_of_exp and rank the prescribers based on their tx_cnt for each state
2024-04-26 13:05:01,063 - Data_transformation - WARNING -data_report2 method executed...., go frwd... 
2024-04-26 13:05:05,498 - root - INFO -extracting files to Output .....
2024-04-26 13:05:05,498 - Extraction - WARNING -extract_files method started executing ....
2024-04-26 13:05:09,470 - Extraction - WARNING -extract_file method successfully executed.....
2024-04-26 13:05:09,470 - Extraction - WARNING -extract_files method started executing ....
2024-04-26 13:05:14,449 - Extraction - WARNING -extract_file method successfully executed.....
2024-04-26 13:05:14,450 - root - INFO -extraction files to output completed..... 
2024-04-26 13:05:14,450 - root - INFO -successfully written into hive
2024-04-26 13:05:14,452 - root - INFO -Now write DataFrame[city: string, state_name: string, county_name: string, population: int, zipcounts: int, presc_count: bigint] into MYSQL 
2024-04-26 13:05:14,452 - Persist - WARNING -executing the data_persist_mysql method... df_city 
2024-04-26 13:06:05,433 - root - INFO -I am the driver applications.........
2024-04-26 13:06:05,433 - root - INFO -Calling the spark object.........  
2024-04-26 13:06:07,056 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x1053cb580>
2024-04-26 13:06:07,057 - root - INFO -validating the spark object
2024-04-26 13:06:08,944 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 26))
2024-04-26 13:06:08,944 - root - WARNING -validation done ! go ahead......
2024-04-26 13:06:08,945 - root - INFO -reading the file format is parquet 
2024-04-26 13:06:08,945 - Ingest - WARNING -load_file method started!...
2024-04-26 13:06:09,275 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-26 13:06:09,277 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-26 13:06:09,688 - root - INFO -validating the dataframe......
2024-04-26 13:06:09,688 - Ingest - WARNING -count the records df_city 
2024-04-26 13:06:09,865 - root - INFO -total number of records in the dataframe 28338
2024-04-26 13:06:09,866 - Ingest - WARNING -load_file method started!...
2024-04-26 13:06:12,421 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-26 13:06:12,423 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-26 13:06:12,537 - root - INFO -validating the dataframe......
2024-04-26 13:06:12,537 - Ingest - WARNING -count the records df_fact 
2024-04-26 13:06:12,929 - root - INFO -total number of records in the dataframe 1329329
2024-04-26 13:06:12,929 - root - INFO -data processing..........
2024-04-26 13:06:12,929 - root - WARNING -data_clean method() started ......
2024-04-26 13:06:12,929 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-26 13:06:12,946 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-26 13:06:12,957 - root - WARNING -Adding a new column to df_presc_sel
2024-04-26 13:06:12,963 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-26 13:06:12,980 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-26 13:06:12,991 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-26 13:06:12,997 - root - WARNING -Checking the null values in all columns
2024-04-26 13:06:13,014 - root - WARNING -successfully dropped the null values.....
2024-04-26 13:06:13,014 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-26 13:06:13,197 - root - INFO -validating schema for dataframes.....
2024-04-26 13:06:13,198 - root - INFO -checking for null values in dataframes... after processing 
2024-04-26 13:06:13,198 - root - INFO -data_transformation executing....
2024-04-26 13:06:13,199 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-26 13:06:13,199 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string] 
2024-04-26 13:06:13,216 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-26 13:06:13,234 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-26 13:06:13,257 - Data_transformation - WARNING -Data_report1 successfully executed .... , go forward
2024-04-26 13:06:13,257 - root - INFO -displaying the df_report_1
2024-04-26 13:06:13,257 - root - INFO -displaying data_report2 method.... 
2024-04-26 13:06:13,257 - Data_transformation - WARNING -executing data_report2 method ...
2024-04-26 13:06:13,257 - Data_transformation - WARNING -executing the task ::: consider the prescribers only from 20 to 50 Years_of_exp and rank the prescribers based on their tx_cnt for each state
2024-04-26 13:06:13,305 - Data_transformation - WARNING -data_report2 method executed...., go frwd... 
2024-04-26 13:06:17,743 - root - INFO -extracting files to Output .....
2024-04-26 13:06:17,744 - Extraction - WARNING -extract_files method started executing ....
2024-04-26 13:06:21,537 - Extraction - WARNING -extract_file method successfully executed.....
2024-04-26 13:06:21,537 - Extraction - WARNING -extract_files method started executing ....
2024-04-26 13:06:26,556 - Extraction - WARNING -extract_file method successfully executed.....
2024-04-26 13:06:26,556 - root - INFO -extraction files to output completed..... 
2024-04-26 13:06:26,556 - root - INFO -successfully written into hive
2024-04-26 13:06:26,558 - root - INFO -Now write DataFrame[city: string, state_name: string, county_name: string, population: int, zipcounts: int, presc_count: bigint] into MYSQL 
2024-04-26 13:06:26,558 - Persist - WARNING -executing the data_persist_mysql method... df_city 
2024-04-26 13:07:53,995 - root - INFO -I am the driver applications.........
2024-04-26 13:07:53,996 - root - INFO -Calling the spark object.........  
2024-04-26 13:07:55,595 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x103962980>
2024-04-26 13:07:55,595 - root - INFO -validating the spark object
2024-04-26 13:07:57,464 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 26))
2024-04-26 13:07:57,464 - root - WARNING -validation done ! go ahead......
2024-04-26 13:07:57,465 - root - INFO -reading the file format is parquet 
2024-04-26 13:07:57,465 - Ingest - WARNING -load_file method started!...
2024-04-26 13:07:57,733 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-26 13:07:57,735 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-26 13:07:58,160 - root - INFO -validating the dataframe......
2024-04-26 13:07:58,160 - Ingest - WARNING -count the records df_city 
2024-04-26 13:07:58,350 - root - INFO -total number of records in the dataframe 28338
2024-04-26 13:07:58,350 - Ingest - WARNING -load_file method started!...
2024-04-26 13:08:01,101 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-26 13:08:01,103 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-26 13:08:01,220 - root - INFO -validating the dataframe......
2024-04-26 13:08:01,220 - Ingest - WARNING -count the records df_fact 
2024-04-26 13:08:01,591 - root - INFO -total number of records in the dataframe 1329329
2024-04-26 13:08:01,591 - root - INFO -data processing..........
2024-04-26 13:08:01,591 - root - WARNING -data_clean method() started ......
2024-04-26 13:08:01,591 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-26 13:08:01,605 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-26 13:08:01,613 - root - WARNING -Adding a new column to df_presc_sel
2024-04-26 13:08:01,619 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-26 13:08:01,633 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-26 13:08:01,645 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-26 13:08:01,653 - root - WARNING -Checking the null values in all columns
2024-04-26 13:08:01,671 - root - WARNING -successfully dropped the null values.....
2024-04-26 13:08:01,671 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-26 13:08:01,896 - root - INFO -validating schema for dataframes.....
2024-04-26 13:08:01,898 - root - INFO -checking for null values in dataframes... after processing 
2024-04-26 13:08:01,898 - root - INFO -data_transformation executing....
2024-04-26 13:08:01,898 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-26 13:08:01,899 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string] 
2024-04-26 13:08:01,916 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-26 13:08:01,933 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-26 13:08:01,957 - Data_transformation - WARNING -Data_report1 successfully executed .... , go forward
2024-04-26 13:08:01,957 - root - INFO -displaying the df_report_1
2024-04-26 13:08:01,957 - root - INFO -displaying data_report2 method.... 
2024-04-26 13:08:01,957 - Data_transformation - WARNING -executing data_report2 method ...
2024-04-26 13:08:01,957 - Data_transformation - WARNING -executing the task ::: consider the prescribers only from 20 to 50 Years_of_exp and rank the prescribers based on their tx_cnt for each state
2024-04-26 13:08:02,004 - Data_transformation - WARNING -data_report2 method executed...., go frwd... 
2024-04-26 13:08:06,512 - root - INFO -extracting files to Output .....
2024-04-26 13:08:06,515 - Extraction - WARNING -extract_files method started executing ....
2024-04-26 13:08:09,990 - Extraction - WARNING -extract_file method successfully executed.....
2024-04-26 13:08:09,990 - Extraction - WARNING -extract_files method started executing ....
2024-04-26 13:08:14,866 - Extraction - WARNING -extract_file method successfully executed.....
2024-04-26 13:08:14,866 - root - INFO -extraction files to output completed..... 
2024-04-26 13:08:14,866 - root - INFO -successfully written into hive
2024-04-26 13:08:14,867 - root - INFO -Now write DataFrame[city: string, state_name: string, county_name: string, population: int, zipcounts: int, presc_count: bigint] into MYSQL 
2024-04-26 13:08:14,867 - Persist - WARNING -executing the data_persist_mysql method... df_city 
2024-04-26 13:10:56,938 - root - INFO -I am the driver applications.........
2024-04-26 13:10:56,938 - root - INFO -Calling the spark object.........  
2024-04-26 13:10:58,533 - root - INFO -object created ......   <pyspark.sql.session.SparkSession object at 0x105bd2980>
2024-04-26 13:10:58,533 - root - INFO -validating the spark object
2024-04-26 13:11:00,480 - root - WARNING -validating spark object on current -- Row(current_date()=datetime.date(2024, 4, 26))
2024-04-26 13:11:00,481 - root - WARNING -validation done ! go ahead......
2024-04-26 13:11:00,481 - root - INFO -reading the file format is parquet 
2024-04-26 13:11:00,481 - Ingest - WARNING -load_file method started!...
2024-04-26 13:11:00,744 - Ingest - WARNING -dataframe created successfully which is parquet -> 
2024-04-26 13:11:00,746 - root - INFO -displaying dataframe DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] 
2024-04-26 13:11:01,409 - root - INFO -validating the dataframe......
2024-04-26 13:11:01,409 - Ingest - WARNING -count the records df_city 
2024-04-26 13:11:01,593 - root - INFO -total number of records in the dataframe 28338
2024-04-26 13:11:01,594 - Ingest - WARNING -load_file method started!...
2024-04-26 13:11:04,318 - Ingest - WARNING -dataframe created successfully which is csv -> 
2024-04-26 13:11:04,319 - root - INFO -displaying dataframe DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] 
2024-04-26 13:11:04,440 - root - INFO -validating the dataframe......
2024-04-26 13:11:04,440 - Ingest - WARNING -count the records df_fact 
2024-04-26 13:11:04,865 - root - INFO -total number of records in the dataframe 1329329
2024-04-26 13:11:04,865 - root - INFO -data processing..........
2024-04-26 13:11:04,865 - root - WARNING -data_clean method() started ......
2024-04-26 13:11:04,865 - root - WARNING -selecting required columns and converting some of columns into upper case...
2024-04-26 13:11:04,880 - root - WARNING -working on OLTP dataset and selecting couple of columns and rename......
2024-04-26 13:11:04,891 - root - WARNING -Adding a new column to df_presc_sel
2024-04-26 13:11:04,896 - root - WARNING -converting Year_of_exp string to Int and replacing = 
2024-04-26 13:11:04,912 - root - WARNING -concatenating of presc_lname & presc_fname
2024-04-26 13:11:04,924 - root - WARNING -dropping the presc_lname & presc_fname
2024-04-26 13:11:04,930 - root - WARNING -Checking the null values in all columns
2024-04-26 13:11:04,943 - root - WARNING -successfully dropped the null values.....
2024-04-26 13:11:04,943 - Data_processing - WARNING -data_clean() method executed done.....
2024-04-26 13:11:05,129 - root - INFO -validating schema for dataframes.....
2024-04-26 13:11:05,130 - root - INFO -checking for null values in dataframes... after processing 
2024-04-26 13:11:05,130 - root - INFO -data_transformation executing....
2024-04-26 13:11:05,130 - Data_transformation - WARNING -processing the data_report1 method..
2024-04-26 13:11:05,131 - Data_transformation - WARNING -calculating total zip counts in DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string] 
2024-04-26 13:11:05,148 - Data_transformation - WARNING -cal distinct prescribers & total tx_cnt
2024-04-26 13:11:05,166 - Data_transformation - WARNING -Don't report city if no prescriber is assigned to it ... | lets join df_city_sel and df_presc_grp 
2024-04-26 13:11:05,191 - Data_transformation - WARNING -Data_report1 successfully executed .... , go forward
2024-04-26 13:11:05,191 - root - INFO -displaying the df_report_1
2024-04-26 13:11:05,191 - root - INFO -displaying data_report2 method.... 
2024-04-26 13:11:05,191 - Data_transformation - WARNING -executing data_report2 method ...
2024-04-26 13:11:05,191 - Data_transformation - WARNING -executing the task ::: consider the prescribers only from 20 to 50 Years_of_exp and rank the prescribers based on their tx_cnt for each state
2024-04-26 13:11:05,235 - Data_transformation - WARNING -data_report2 method executed...., go frwd... 
2024-04-26 13:11:09,468 - root - INFO -extracting files to Output .....
2024-04-26 13:11:09,471 - Extraction - WARNING -extract_files method started executing ....
2024-04-26 13:11:13,048 - Extraction - WARNING -extract_file method successfully executed.....
2024-04-26 13:11:13,049 - Extraction - WARNING -extract_files method started executing ....
2024-04-26 13:11:18,321 - Extraction - WARNING -extract_file method successfully executed.....
2024-04-26 13:11:18,321 - root - INFO -extraction files to output completed..... 
2024-04-26 13:11:18,321 - root - INFO -successfully written into hive
2024-04-26 13:11:18,323 - root - INFO -Now write DataFrame[city: string, state_name: string, county_name: string, population: int, zipcounts: int, presc_count: bigint] into MYSQL 
2024-04-26 13:11:18,323 - Persist - WARNING -executing the data_persist_mysql method... df_city 
2024-04-26 13:11:22,145 - Persist - WARNING -Persist_data_mysql method executing successfully .. into city_df 
2024-04-26 13:11:22,145 - root - INFO -total amount of time taken via process : 25.21 seconds
2024-04-26 13:11:22,145 - root - INFO -Applications name .......   
